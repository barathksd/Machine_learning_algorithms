{"nbformat_minor": 1, "cells": [{"source": "This is the last assignment for the Coursera course \"Advanced Machine Learning and Signal Processing\"\n\nJust execute all cells one after the other and you are done - just note that in the last one you should update your email address (the one you've used for coursera) and obtain a submission token, you get this from the programming assignment directly on coursera.\n\nPlease fill in the sections labelled with \"###YOUR_CODE_GOES_HERE###\"\n\nThe purpose of this assignment is to learn how feature engineering boosts model performance. You will apply Discrete Fourier Transformation on the accelerometer sensor time series and therefore transforming the dataset from the time to the frequency domain. \n\nAfter that, you\u2019ll use a classification algorithm of your choice to create a model and submit the new predictions to the grader. Done.\n", "cell_type": "markdown", "metadata": {}}, {"source": "credentials_1 = {'password':\"\"\"4b5403df0d792637f53845b5b93c089d8fc7f225ba85306f0deeaa4518df1804\"\"\",\n                 'custom_url':'https://ab28a05d-e0f8-43a2-9930-cfa9e2737b8f-bluemix:4b5403df0d792637f53845b5b93c089d8fc7f225ba85306f0deeaa4518df1804@ab28a05d-e0f8-43a2-9930-cfa9e2737b8f-bluemix.cloudant.com',\n                 'username':'ab28a05d-e0f8-43a2-9930-cfa9e2737b8f-bluemix' }\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190313115125-0000\nKERNEL_ID = 2749ebe4-f337-407e-b56f-1b0b133cf4e0\n"}], "execution_count": 1}, {"source": "Let's create a SparkSession object and put the Cloudant credentials into it", "cell_type": "markdown", "metadata": {}}, {"source": "spark = SparkSession\\\n    .builder\\\n    .appName(\"Cloudant Spark SQL Example in Python using temp tables\")\\\n    .config(\"cloudant.host\",credentials_1['custom_url'].split('@')[1])\\\n    .config(\"cloudant.username\", credentials_1['username'])\\\n    .config(\"cloudant.password\",credentials_1['password'])\\\n    .getOrCreate()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 2}, {"source": "Now it\u2019s time to read the sensor data and create a temporary query table.", "cell_type": "markdown", "metadata": {}}, {"source": "df=spark.read.load('shake_classification', \"org.apache.bahir.cloudant\")\ndf.createOrReplaceTempView(\"df\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 3}, {"source": "We need to make sure SystemML is installed.\n", "cell_type": "markdown", "metadata": {}}, {"source": "!pip install py4j\n!pip install pyspark\n!pip install systemml", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Collecting py4j\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/de/2d314a921ef4c20b283e1de94e0780273678caac901564df06b948e4ba9b/py4j-0.10.8.1-py2.py3-none-any.whl (196kB)\n\u001b[K    100% |################################| 204kB 2.3MB/s eta 0:00:01\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n\u001b[31mpyspark 2.3.0 has requirement py4j==0.10.6, but you'll have py4j 0.10.8.1 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: py4j\nSuccessfully installed py4j-0.10.8.1\nCollecting pyspark\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n\u001b[K    100% |################################| 213.4MB 84kB/s  eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K    100% |################################| 204kB 3.5MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\nSuccessfully built pyspark\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.0\n\u001b[33mTarget directory /home/spark/shared/user-libs/python2/py4j already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python2/share already exists. Specify --upgrade to force replacement.\u001b[0m\nCollecting systemml\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/94/62104cb8c526b462cd501c7319926fb81ac9a5668574a0b3407658a506ab/systemml-1.2.0.tar.gz (9.7MB)\n\u001b[K    100% |################################| 9.7MB 1.3MB/s eta 0:00:01\n\u001b[?25hCollecting numpy>=1.8.2 (from systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/33/8ec8dcdb4ede5d453047bbdbd01916dbaccdb63e98bba60989718f5f0876/numpy-1.16.2-cp27-cp27mu-manylinux1_x86_64.whl (17.0MB)\n\u001b[K    100% |################################| 17.0MB 826kB/s eta 0:00:01\n\u001b[?25hCollecting scipy>=0.15.1 (from systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/39/f1457091d0a45a84a2bd7815e2cf6bd45d4fe240728e9ed567cbb17c8abe/scipy-1.2.1-cp27-cp27mu-manylinux1_x86_64.whl (24.8MB)\n\u001b[K    100% |################################| 24.8MB 655kB/s eta 0:00:01\n\u001b[?25hCollecting pandas (from systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/88/b8659eecde0350d37d5b47c1c2a88f39e6153e5809bcfc48bb7fde6f231b/pandas-0.24.1-cp27-cp27mu-manylinux1_x86_64.whl (10.1MB)\n\u001b[K    100% |################################| 10.1MB 1.6MB/s eta 0:00:01\n\u001b[?25hCollecting scikit-learn (from systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/bb/52a01390c1dbb2c65d3072bc687271aa9ddf6964141ce7e03304820138f4/scikit_learn-0.20.3-cp27-cp27mu-manylinux1_x86_64.whl (5.5MB)\n\u001b[K    100% |################################| 5.5MB 2.6MB/s eta 0:00:01\n\u001b[?25hCollecting Pillow>=2.0.0 (from systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/f3/421598450cb9503f4565d936860763b5af413a61009d87a5ab1e34139672/Pillow-5.4.1-cp27-cp27mu-manylinux1_x86_64.whl (2.0MB)\n\u001b[K    100% |################################| 2.0MB 3.6MB/s eta 0:00:01\n\u001b[?25hCollecting pytz>=2011k (from pandas->systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl (510kB)\n\u001b[K    100% |################################| 512kB 4.3MB/s eta 0:00:01\n\u001b[?25hCollecting python-dateutil>=2.5.0 (from pandas->systemml)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n\u001b[K    100% |################################| 235kB 4.2MB/s eta 0:00:01\n\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.5.0->pandas->systemml)\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nBuilding wheels for collected packages: systemml\n  Running setup.py bdist_wheel for systemml ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/cf/07/79/b3ed6f12afe06b2ab55d60dcfd62e66240f5d8c6088a518177\nSuccessfully built systemml\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n\u001b[31mpyspark 2.4.0 has requirement py4j==0.10.7, but you'll have py4j 0.10.8.1 which is incompatible.\u001b[0m\nInstalling collected packages: numpy, scipy, pytz, six, python-dateutil, pandas, scikit-learn, Pillow, systemml\nSuccessfully installed Pillow-5.4.1 numpy-1.16.2 pandas-0.24.1 python-dateutil-2.8.0 pytz-2018.9 scikit-learn-0.20.3 scipy-1.2.1 six-1.12.0 systemml-1.2.0\n\u001b[33mTarget directory /home/spark/shared/user-libs/python2/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"}], "execution_count": 4}, {"source": "We\u2019ll use Apache SystemML to implement Discrete Fourier Transformation. This way all computation continues to happen on the Apache Spark cluster for advanced scalability and performance.", "cell_type": "markdown", "metadata": {}}, {"source": "from systemml import MLContext, dml\nml = MLContext(spark)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 5}, {"source": "As you\u2019ve learned from the lecture, implementing Discrete Fourier Transformation in a linear algebra programming language is simple. Apache SystemML DML is such a language and as you can see the implementation is straightforward and doesn\u2019t differ too much from the mathematical definition (Just note that the sum operator has been swapped with a vector dot product using the %*% syntax borrowed from R\n):\n\n<img style=\"float: left;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1af0a78dc50bbf118ab6bd4c4dcc3c4ff8502223\">\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "dml_script = '''\nPI = 3.141592654\nN = nrow(signal)\n\nn = seq(0, N-1, 1)\nk = seq(0, N-1, 1)\n\nM = (n %*% t(k))*(2*PI/N)\n\nXa = cos(M) %*% signal\nXb = sin(M) %*% signal\n\nDFT = cbind(Xa, Xb)\n'''", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 6}, {"source": "Now it\u2019s time to create a function which takes a single row Apache Spark data frame as argument (the one containing the accelerometer measurement time series for one axis) and returns the Fourier transformation of it. In addition, we are adding an index column for later joining all axis together and renaming the columns to appropriate names. The result of this function is an Apache Spark DataFrame containing the Fourier Transformation of its input in two columns. \n", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.sql.functions import monotonically_increasing_id\n\ndef dft_systemml(signal,name):\n    prog = dml(dml_script).input('signal', signal).output('DFT')\n    \n    return (\n\n    #execute the script inside the SystemML engine running on top of Apache Spark\n    ml.execute(prog) \n     \n         #read result from SystemML execution back as SystemML Matrix\n        .get('DFT') \n     \n         #convert SystemML Matrix to ApacheSpark DataFrame \n        .toDF() \n     \n         #rename default column names\n        .selectExpr('C1 as %sa' % (name), 'C2 as %sb' % (name)) \n     \n         #add unique ID per row for later joining\n        .withColumn(\"id\", monotonically_increasing_id())\n    )\n        \n\n\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "Now it\u2019s time to create DataFrames containing for each accelerometer sensor axis and one for each class. This means you\u2019ll get 6 DataFrames. Please implement this using the relational API of DataFrames or SparkSQL.\n", "cell_type": "markdown", "metadata": {}}, {"source": "x0 = spark.sql('select x from df where class =0') ###YOUR_CODE_GOES_HERE### => Please create a DataFrame containing only measurements of class 0 from the x axis\ny0 = spark.sql('select y from df where class =0') ###YOUR_CODE_GOES_HERE### => Please create a DataFrame containing only measurements of class 0 from the y axis\nz0 = spark.sql('select z from df where class =0') ###YOUR_CODE_GOES_HERE### => Please create a DataFrame containing only measurements of class 0 from the z axis\nx1 = spark.sql('select x from df where class =1') ###YOUR_CODE_GOES_HERE### => Please create a DataFrame containing only measurements of class 1 from the x axis\ny1 = spark.sql('select y from df where class =1') ###YOUR_CODE_GOES_HERE### => Please create a DataFrame containing only measurements of class 1 from the y axis\nz1 = spark.sql('select z from df where class =1') ###YOUR_CODE_GOES_HERE### => Please create a DataFrame containing only measurements of class 1 from the z axis", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "Since we\u2019ve created this cool DFT function before, we can just call it for each of the 6 DataFrames now. And since the result of this function call is a DataFrame again we can use the pyspark best practice in simply calling methods on it sequentially. So what we are doing is the following:\n\n- Calling DFT for each class and accelerometer sensor axis.\n- Joining them together on the ID column. \n- Re-adding a column containing the class index.\n- Stacking both Dataframes for each classes together\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.sql.functions import lit\n\ndf_class_0 = dft_systemml(x0,'x') \\\n    .join(dft_systemml(y0,'y'), on=['id'], how='inner') \\\n    .join(dft_systemml(z0,'z'), on=['id'], how='inner') \\\n    .withColumn('class', lit(0))\n    \ndf_class_1 = dft_systemml(x1,'x') \\\n    .join(dft_systemml(y1,'y'), on=['id'], how='inner') \\\n    .join(dft_systemml(z1,'z'), on=['id'], how='inner') \\\n    .withColumn('class', lit(1))\n\ndf_dft = df_class_0.union(df_class_1)\n\ndf_dft.show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "'JavaPackage' object is not callable", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-9-ea0255d93ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_class_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft_systemml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdft_systemml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdft_systemml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_class_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft_systemml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdft_systemml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdft_systemml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-7-bba9425aa9d2>\u001b[0m in \u001b[0;36mdft_systemml\u001b[0;34m(signal, name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#execute the script inside the SystemML engine running on top of Apache Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m          \u001b[0;31m#read result from SystemML execution back as SystemML Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/spark/shared/user-libs/python2/systemml/mlcontext.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, script)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mdefault_jvm_stdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_jvm_stdout_parallel_flush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefault_jvm_stdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mjvm_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_flush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_jvm_stdout_parallel_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMLResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_java\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/spark/shared/user-libs/python2/systemml/classloader.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parallel_flush)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_flush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msysml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUtils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_flush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_flush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_stdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"], "ename": "TypeError"}], "execution_count": 9}, {"source": "Please create a VectorAssembler which consumes the newly created DFT columns and produces a column \u201cfeatures\u201d\n", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler(inputCols=[\"X\",\"Y\",\"Z\"],\n                                  outputCol=\"features\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "Please insatiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to set the \u201cclass\u201d column as target.\n", "cell_type": "markdown", "metadata": {}}, {"source": "\nfrom pyspark.ml.classification import RandomForestClassifier\nclassifier = RandomForestClassifier(maxDepth=8, maxBins=2400000, numTrees=128,impurity=\"gini\") ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 15}, {"source": "Let\u2019s train and evaluate\u2026\n", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, classifier])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 16}, {"source": "model = pipeline.fit(df_dft)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "name 'df_dft' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-17-2691210881cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'df_dft' is not defined"], "ename": "NameError"}], "execution_count": 17}, {"source": "prediction = model.transform(df_dft)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "name 'model' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-18-33fe48dffaed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"], "ename": "NameError"}], "execution_count": 18}, {"source": "prediction.show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "name 'prediction' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-19-93f56211762f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"], "ename": "NameError"}], "execution_count": 19}, {"source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"class\")\n    \nbinEval.evaluate(prediction) ", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "name 'prediction' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-20-e186a8762361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbinEval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMetricName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msetPredictionCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLabelCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbinEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"], "ename": "NameError"}], "execution_count": 20}, {"source": "If you are happy with the result (I\u2019m happy with > 0.8) please submit your solution to the grader by executing the following cells, please don\u2019t forget to obtain an assignment submission token (secret) from the Courera\u2019s graders web page and paste it to the \u201csecret\u201d variable below, including your email address you\u2019ve used for Coursera. \n", "cell_type": "markdown", "metadata": {}}, {"source": "!rm -Rf a2_m4.parquet", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 21}, {"source": "prediction = prediction.repartition(1)\nprediction.write.json('a2_m4.json')", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "name 'prediction' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-22-807b8f13c3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a2_m4.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"], "ename": "NameError"}], "execution_count": 22}, {"source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2019-03-13 12:01:24--  https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2289 (2.2K) [text/plain]\nSaving to: 'rklib.py'\n\nrklib.py            100%[===================>]   2.24K  --.-KB/s    in 0s      \n\n2019-03-13 12:01:24 (47.9 MB/s) - 'rklib.py' saved [2289/2289]\n\n"}], "execution_count": 23}, {"source": "import zipfile\n\ndef zipdir(path, ziph):\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file))\n\nzipf = zipfile.ZipFile('a2_m4.json.zip', 'w', zipfile.ZIP_DEFLATED)\nzipdir('a2_m4.json', zipf)\nzipf.close()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 24}, {"source": "!base64 a2_m4.json.zip > a2_m4.json.zip.base64", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 25}, {"source": "from rklib import submit\nkey = \"-fBiYHYDEeiR4QqiFhAvkA\"\npart = \"IjtJk\"\nemail = \"bksderode@gmail.com\"\nsecret = \"zKjpxXFPAtRnifS5\"\n\nwith open('a2_m4.json.zip.base64', 'r') as myfile:\n    data=myfile.read()\nsubmit(email, secret, key, part, [part], data)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Submission successful, please check on the coursera grader page for the status\n-------------------------\n{\"elements\":[{\"itemId\":\"B8wXV\",\"id\":\"f_F-qCtuEei_fRLwaVDk3g~B8wXV~2ZkTTkWHEemJ5xJ6kj-nZg\",\"courseId\":\"f_F-qCtuEei_fRLwaVDk3g\"}],\"paging\":{},\"linked\":{}}\n-------------------------\n"}], "execution_count": 26}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 2.7 with Spark", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "pygments_lexer": "ipython2", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "nbformat": 4}